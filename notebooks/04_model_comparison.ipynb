# 04_model_comparison.ipynb

# ‚úÖ Compare different NER models (XLM-R, mBERT, DistilBERT) and log metrics

from transformers import AutoModelForTokenClassification, Trainer
from datasets import load_metric
import numpy as np

# Define model names to compare
models = [
    "Davlan/bert-base-multilingual-cased-ner",
    "Davlan/distilbert-base-multilingual-cased-ner-hrl",
    "Davlan/xlm-roberta-base-ner-hrl"
]

results = {}

for model_name in models:
    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))
    trainer.model = model  # Reuse the existing trainer config
    trainer.train()
    metrics = trainer.evaluate()
    results[model_name] = metrics

# Display all comparison results
for name, score in results.items():
    print(f"\nüìä Results for {name}")
    for k, v in score.items():
        if isinstance(v, float):
            print(f"{k}: {v:.4f}")

# Select best model
best_model = max(results.items(), key=lambda x: x[1]["eval_f1"])[0]
print(f"\nüèÜ Best Model: {best_model}")
