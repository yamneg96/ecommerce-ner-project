# 03_model_finetuning.ipynb

# âœ… Fine-tune NER model for Amharic entity extraction

from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import precision_recall_fscore_support
import os

# --- Step 1: Load CoNLL-labeled data ---
conll_path = "../data/labeled_data.conll"

def load_conll_data(filepath):
    sentences, labels = [], []
    sentence, label = [], []
    with open(filepath, 'r', encoding='utf-8') as file:
        for line in file:
            line = line.strip()
            if not line:
                if sentence:
                    sentences.append(sentence)
                    labels.append(label)
                    sentence, label = [], []
            else:
                token, tag = line.split()
                sentence.append(token)
                label.append(tag)
    return Dataset.from_dict({"tokens": sentences, "ner_tags": labels})

dataset = load_conll_data(conll_path)
dataset = dataset.train_test_split(test_size=0.2)

# --- Step 2: Tokenize and align labels ---
model_ckpt = "Davlan/bert-base-multilingual-cased-ner"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
label_list = sorted(list({tag for row in dataset['train']["ner_tags"] for tag in row}))
label_to_id = {l: i for i, l in enumerate(label_list)}
id_to_label = {i: l for l, i in label_to_id.items()}

def tokenize_and_align(example):
    tokenized = tokenizer(example["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    word_ids = tokenized.word_ids()
    prev_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            labels.append(-100)
        elif word_idx != prev_word_idx:
            labels.append(label_to_id[example["ner_tags"][word_idx]])
        else:
            labels.append(label_to_id[example["ner_tags"][word_idx]])
        prev_word_idx = word_idx
    tokenized["labels"] = labels
    return tokenized

tokenized_ds = dataset.map(tokenize_and_align)

# --- Step 3: Prepare training ---
model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=len(label_list))
data_collator = DataCollatorForTokenClassification(tokenizer)

args = TrainingArguments(
    output_dir="../models/fine_tuned_model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="../logs"
)

def compute_metrics(pred):
    labels = pred.label_ids.flatten()
    preds = np.argmax(pred.predictions, axis=-1).flatten()
    valid = labels != -100
    return {k: v for k, v in zip(['precision', 'recall', 'f1', 'support'], precision_recall_fscore_support(labels[valid], preds[valid], average="macro"))}

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# --- Step 4: Train and Save ---
trainer.train()
model.save_pretrained("../models/fine_tuned_model")
tokenizer.save_pretrained("../models/fine_tuned_model")
